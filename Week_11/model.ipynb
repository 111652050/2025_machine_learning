{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbd738d2-ab17-4e74-b271-9c77dc6f0e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   state action  next_state  reward\n",
      "0      0    run           1    -1.0\n",
      "1      1    run           2    -1.0\n",
      "2      2   jump           3    -1.0\n",
      "3      3    run           4    -1.0\n",
      "4      4   jump           5    -1.0\n",
      "5      5    run           6    -1.0\n",
      "6      6    run           7    -1.0\n",
      "7      7    run           8    -1.0\n",
      "8      8    run           9   100.0\n",
      "\n",
      "Total reward: 92.0\n",
      "Steps: 9\n",
      "\n",
      "Optimal policy per tile: {0: 'run', 1: 'run', 2: 'jump', 3: 'run', 4: 'jump', 5: 'run', 6: 'run', 7: 'run', 8: 'run', 9: None}\n"
     ]
    }
   ],
   "source": [
    "#極簡化的模型(不考慮各種glitch)\n",
    "\n",
    "# Simplified \"Mario-like\" 1D runner MDP solved via dynamic programming (value iteration).\n",
    "# --- ENVIRONMENT DESCRIPTION ---\n",
    "# States: positions 0..9\n",
    "# Actions: \"run\" (go forward), \"jump\" (go forward but avoids hazard)\n",
    "# Hazards: pit at tile 3, enemy at tile 5 (run into them => death)\n",
    "# Rewards: -1 every step, +100 reach goal, -100 death\n",
    "# Goal: tile 9\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "tiles = list(range(10))\n",
    "start = 0\n",
    "goal = 9\n",
    "pits = {3}\n",
    "enemies = {5}\n",
    "actions = [\"run\", \"jump\"]\n",
    "\n",
    "states = tiles + [\"DEAD\"]\n",
    "terminal_states = {goal, \"DEAD\"}\n",
    "\n",
    "def next_state_and_reward(s: int, a: str) -> Tuple[object, float]:\n",
    "    if s in terminal_states:\n",
    "        return s, 0.0\n",
    "    target = s + 1\n",
    "    if a == \"run\":\n",
    "        if target in pits or target in enemies:\n",
    "            return \"DEAD\", -100.0\n",
    "        return (goal, 100.0) if target == goal else (target, -1.0)\n",
    "    if a == \"jump\":\n",
    "        return (goal, 100.0) if target == goal else (target, -1.0)\n",
    "\n",
    "P = {s: {a: next_state_and_reward(s, a) for a in actions} for s in tiles}\n",
    "P[\"DEAD\"] = {a: (\"DEAD\", 0.0) for a in actions}\n",
    "P[goal] = {a: (goal, 0.0) for a in actions}\n",
    "\n",
    "gamma = 1.0\n",
    "V = {s: 0.0 for s in states}\n",
    "\n",
    "def value_iteration(P, V, actions, theta=1e-6, max_iters=1000):\n",
    "    for _ in range(max_iters):\n",
    "        delta = 0.0\n",
    "        V_new = V.copy()\n",
    "        for s in tiles:\n",
    "            if s in terminal_states: \n",
    "                continue\n",
    "            Qs = [P[s][a][1] + gamma * V[P[s][a][0]] for a in actions]\n",
    "            V_new[s] = max(Qs)\n",
    "            delta = max(delta, abs(V_new[s] - V[s]))\n",
    "        V = V_new\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V = value_iteration(P, V, actions)\n",
    "\n",
    "policy = {}\n",
    "for s in tiles:\n",
    "    if s in terminal_states:\n",
    "        policy[s] = None\n",
    "        continue\n",
    "    best_a = max(actions, key=lambda a: P[s][a][1] + V[P[s][a][0]])\n",
    "    policy[s] = best_a\n",
    "\n",
    "trajectory = []\n",
    "s = start\n",
    "while s not in terminal_states:\n",
    "    a = policy[s]\n",
    "    s_next, r = P[s][a]\n",
    "    trajectory.append((s, a, s_next, r))\n",
    "    s = s_next\n",
    "\n",
    "df = pd.DataFrame(trajectory, columns=[\"state\",\"action\",\"next_state\",\"reward\"])\n",
    "print(df)\n",
    "print(\"\\nTotal reward:\", df[\"reward\"].sum())\n",
    "print(\"Steps:\", len(df))\n",
    "print(\"\\nOptimal policy per tile:\", policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b6bebec-1b47-48b5-9651-2ba88fdf2ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Explorer] Found bullet bill at iter 19071, steps=145, total_trace_len=182\n",
      "\n",
      "Archive size: 121\n",
      "Found bullet bill? True\n",
      "Readable trace: ['noop', 'noop', 'right', 'right', 'right', 'jump', 'right+jump', 'noop', 'jump', 'jump', 'jump', 'jump', 'jump', 'right', 'noop', 'right+jump', 'jump', 'noop', 'noop', 'noop', 'noop', 'right+jump', 'jump', 'jump', 'noop', 'right', 'right', 'noop', 'noop', 'jump', 'jump', 'noop', 'jump', 'jump', 'jump', 'noop', 'right', 'jump', 'right', 'noop', 'jump', 'jump', 'noop', 'right+jump', 'noop', 'right', 'right+jump', 'noop', 'right', 'noop', 'jump', 'right', 'jump', 'jump', 'jump', 'jump', 'jump', 'right', 'noop', 'noop', 'noop', 'noop', 'right', 'noop', 'noop', 'right', 'jump', 'right+jump', 'right', 'noop', 'noop', 'right+jump', 'noop', 'noop', 'right', 'jump', 'right', 'right', 'right+jump', 'jump', 'right+jump', 'noop', 'jump', 'noop', 'jump', 'jump', 'jump', 'right+jump', 'noop', 'noop', 'noop', 'jump', 'jump', 'right', 'right', 'noop', 'right', 'noop', 'right', 'right+jump', 'noop', 'jump', 'jump', 'jump', 'noop', 'right', 'noop', 'jump', 'noop', 'jump', 'jump', 'jump', 'noop', 'jump', 'noop', 'jump', 'right', 'right', 'right', 'jump', 'right+jump', 'noop', 'jump', 'right', 'jump', 'noop', 'jump', 'noop', 'jump', 'noop', 'jump', 'right', 'jump', 'right', 'jump', 'noop', 'jump', 'jump', 'noop', 'right+jump', 'noop', 'jump', 'noop', 'right', 'jump', 'jump', 'right', 'right', 'right+jump', 'right', 'jump', 'right+jump', 'right+jump', 'noop', 'jump', 'noop', 'right+jump', 'jump', 'right', 'noop', 'noop', 'jump', 'jump', 'right', 'noop', 'right+jump', 'right', 'jump', 'noop', 'right+jump', 'noop', 'noop', 'noop', 'noop', 'jump', 'jump', 'jump', 'jump', 'right+jump', 'jump', 'right', 'right+jump']\n",
      "Final obs: {'pos': 14, 'subpixel': 0.5, 'ram': (8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), 'frame': 182, 'special_positions': (14,)}\n",
      "Info: {'warp_byte': 8, 'collided_special': True}\n"
     ]
    }
   ],
   "source": [
    "#以Bullet Bill為例，說明發現glitch的過程\n",
    "# bullet_bill_demo.py\n",
    "# Simplified \"Bullet Bill\" model (Design B) + lightweight Go-Explore style search\n",
    "# - BulletBillEnv: deterministic env with RAM and save/load\n",
    "# - RandomExplorer: performs archive-based exploration to try to find warp_byte != expected (bullet bill)\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import random, hashlib, json\n",
    "from typing import List, Any\n",
    "\n",
    "@dataclass\n",
    "class BulletBillEnv:\n",
    "    level_length: int = 30\n",
    "    goal_pos: int = 29\n",
    "    player_start: int = 0\n",
    "    warp_addr: int = 0\n",
    "    ram_size: int = 16\n",
    "    rng_seed: int = 12345\n",
    "    \n",
    "    pos: int = field(init=False, default=0)\n",
    "    subpixel: float = field(init=False, default=0.0)\n",
    "    frame: int = field(init=False, default=0)\n",
    "    ram: List[int] = field(init=False, default_factory=list)\n",
    "    rng_state: Any = field(init=False, default=None)\n",
    "    action_history: List[int] = field(init=False, default_factory=list)\n",
    "    done: bool = field(init=False, default=False)\n",
    "    info: dict = field(init=False, default_factory=dict)\n",
    "    \n",
    "    def reset(self, seed: int = None):\n",
    "        if seed is None:\n",
    "            seed = self.rng_seed\n",
    "        self.rng_state = random.Random(seed)\n",
    "        self.pos = self.player_start\n",
    "        self.subpixel = 0.0\n",
    "        self.frame = 0\n",
    "        self.ram = [0] * self.ram_size\n",
    "        self.ram[self.warp_addr] = 1\n",
    "        self.action_history = []\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        self.special_positions = self._precompute_special_positions()\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def _precompute_special_positions(self):\n",
    "        s = {}\n",
    "        r = self.rng_state\n",
    "        for f in range(0, 200):\n",
    "            if f % 7 == 0:\n",
    "                p = (5 + (f * 3 + r.randint(0,3)) % (self.level_length - 10))\n",
    "                s[f] = {p}\n",
    "            else:\n",
    "                s[f] = set()\n",
    "        return s\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"pos\": self.pos,\n",
    "            \"subpixel\": round(self.subpixel,3),\n",
    "            \"ram\": tuple(self.ram),\n",
    "            \"frame\": self.frame,\n",
    "            \"special_positions\": tuple(sorted(self.special_positions.get(self.frame, set())))\n",
    "        }\n",
    "    \n",
    "    def save_state(self):\n",
    "        return {\n",
    "            \"pos\": self.pos,\n",
    "            \"subpixel\": self.subpixel,\n",
    "            \"frame\": self.frame,\n",
    "            \"ram\": list(self.ram),\n",
    "            \"rng_state\": self.rng_state.getstate(),\n",
    "            \"action_history\": list(self.action_history)\n",
    "        }\n",
    "    \n",
    "    def load_state(self, state):\n",
    "        self.pos = state[\"pos\"]\n",
    "        self.subpixel = state[\"subpixel\"]\n",
    "        self.frame = state[\"frame\"]\n",
    "        self.ram = list(state[\"ram\"])\n",
    "        r = random.Random()\n",
    "        r.setstate(state[\"rng_state\"])\n",
    "        self.rng_state = r\n",
    "        self.action_history = list(state[\"action_history\"])\n",
    "        self.done = False\n",
    "        self.info = {}\n",
    "        self.special_positions = self._precompute_special_positions()\n",
    "        return self._get_obs()\n",
    "    \n",
    "    def step(self, action: int):\n",
    "        if self.done:\n",
    "            return self._get_obs(), 0.0, True, self.info\n",
    "        self.frame += 1\n",
    "        self.action_history.append(action)\n",
    "        \n",
    "        # motion model: action -> subpixel advance\n",
    "        if action == 0:\n",
    "            adv = 0.0\n",
    "        elif action == 1:\n",
    "            adv = 0.25\n",
    "        elif action == 2:\n",
    "            adv = 0.25\n",
    "        elif action == 3:\n",
    "            adv = 0.0\n",
    "        else:\n",
    "            adv = 0.0\n",
    "        self.subpixel += adv\n",
    "        while self.subpixel >= 1.0:\n",
    "            self.subpixel -= 1.0\n",
    "            self.pos += 1\n",
    "        if self.pos >= self.level_length:\n",
    "            self.pos = self.level_length - 1\n",
    "        \n",
    "        collided = False\n",
    "        special_here = self.special_positions.get(self.frame, set())\n",
    "        if self.pos in special_here:\n",
    "            # glitch trigger condition: exact subpixel == 0.5 and action == 2\n",
    "            if abs(self.subpixel - 0.5) < 1e-9 and action == 2:\n",
    "                self.ram[self.warp_addr] = 8  # Bullet Bill trigger (corrupt warp byte)\n",
    "                collided = True\n",
    "            else:\n",
    "                # normal collision effect: knockback + minor RAM side-effect\n",
    "                self.pos = max(0, self.pos - 1)\n",
    "                self.ram[5] = (self.ram[5] + 1) % 256\n",
    "        \n",
    "        reward = -1.0\n",
    "        done = False\n",
    "        if self.pos >= self.goal_pos:\n",
    "            reward += 200.0\n",
    "            done = True\n",
    "            self.done = True\n",
    "        \n",
    "        info = {\"warp_byte\": self.ram[self.warp_addr], \"collided_special\": collided}\n",
    "        self.info = info\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "def state_hash(state_obs):\n",
    "    ram = state_obs[\"ram\"]\n",
    "    key = (ram[0], ram[1], ram[2], state_obs[\"pos\"], round(state_obs[\"subpixel\"],3))\n",
    "    return hashlib.md5(json.dumps(key).encode()).hexdigest()\n",
    "\n",
    "class RandomExplorer:\n",
    "    def __init__(self, env: BulletBillEnv, max_iters=500, explore_steps=30):\n",
    "        self.env = env\n",
    "        self.archive = {}\n",
    "        self.max_iters = max_iters\n",
    "        self.explore_steps = explore_steps\n",
    "        self.found_traces = []\n",
    "    def initialize(self, seed=123):\n",
    "        obs = self.env.reset(seed)\n",
    "        s = self.env.save_state()\n",
    "        h = state_hash(obs)\n",
    "        self.archive[h] = {\"save\": s, \"obs\": obs, \"trace\": []}\n",
    "    def pick_entry(self):\n",
    "        return random.choice(list(self.archive.keys()))\n",
    "    def explore(self):\n",
    "        for it in range(self.max_iters):\n",
    "            h = self.pick_entry()\n",
    "            entry = self.archive[h]\n",
    "            env_state = entry[\"save\"]\n",
    "            obs = self.env.load_state(env_state)\n",
    "            local_trace = []\n",
    "            found = False\n",
    "            for step in range(self.explore_steps):\n",
    "                if random.random() < 0.12:\n",
    "                    action = 2\n",
    "                else:\n",
    "                    action = random.choice([0,1,3])\n",
    "                obs, rew, done, info = self.env.step(action)\n",
    "                local_trace.append(action)\n",
    "                h2 = state_hash(obs)\n",
    "                if h2 not in self.archive:\n",
    "                    self.archive[h2] = {\"save\": self.env.save_state(), \"obs\": obs, \"trace\": entry[\"trace\"] + local_trace.copy()}\n",
    "                if info.get(\"warp_byte\", None) != 1:\n",
    "                    found = True\n",
    "                    self.found_traces.append({\"origin\": h, \"steps\": step+1, \"trace\": entry[\"trace\"] + local_trace.copy(), \"obs\": obs, \"info\": info})\n",
    "                    h_new = state_hash(obs)\n",
    "                    self.archive[h_new] = {\"save\": self.env.save_state(), \"obs\": obs, \"trace\": entry[\"trace\"] + local_trace.copy()}\n",
    "                    break\n",
    "                if done:\n",
    "                    break\n",
    "            if found:\n",
    "                print(f\"[Explorer] Found bullet bill at iter {it}, steps={step+1}, total_trace_len={len(entry['trace'])+len(local_trace)}\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run a short demo with limited budget\n",
    "    env = BulletBillEnv(rng_seed=2025)\n",
    "    expl = RandomExplorer(env, max_iters=100000, explore_steps=2000)\n",
    "    expl.initialize(seed=2025)\n",
    "    found = expl.explore()\n",
    "\n",
    "    print(\"\\nArchive size:\", len(expl.archive))\n",
    "    print(\"Found bullet bill?\", found)\n",
    "    if found:\n",
    "        rec = expl.found_traces[-1]\n",
    "        act_map = {0:\"noop\",1:\"right\",2:\"right+jump\",3:\"jump\"}\n",
    "        print(\"Readable trace:\", [act_map[a] for a in rec[\"trace\"]])\n",
    "        print(\"Final obs:\", rec[\"obs\"])\n",
    "        print(\"Info:\", rec[\"info\"])\n",
    "    else:\n",
    "        print(\"No bullet bill found in budget. Increase max_iters/explore_steps or bias action 2 more.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c607d6-0a96-43b4-b806-5867f77c187e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
